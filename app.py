# -*- coding: utf-8 -*-
"""app1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OYzqbIG63wZUcJdEnSouG8eIJDjD8iNB
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
import streamlit as st
import pickle
import seaborn as sns
import matplotlib.pyplot as plt
import os
import time
from datetime import datetime, timedelta
from threading import Thread

# Load the KMeans model
try:
    model = pickle.load(open('./kmeans_model.pkl', 'rb'))
except Exception as e:
    st.error(f"Error loading model: {e}")
    st.stop()

def load_and_clean_data(file_path):
    try:
        retail = pd.read_csv(file_path, sep=',', encoding="ISO-8859-1", header=0)
        retail['CustomerID'] = retail['CustomerID'].astype(str)
        retail['CustomerID'] = retail['CustomerID'].apply(lambda x: x.split('.')[0])
        retail['Amount'] = retail['Quantity'] * retail['UnitPrice']
        rfm_m = retail.groupby('CustomerID')['Amount'].sum().reset_index()
        rfm_f = retail.groupby('CustomerID')['InvoiceNo'].count().reset_index()
        rfm_f.columns = ['CustomerID', 'Frequency']
        retail['InvoiceDate'] = pd.to_datetime(retail['InvoiceDate'], format='%Y-%m-%d %H:%M:%S.%f')
        max_date = max(retail['InvoiceDate'])
        retail['Diff'] = max_date - retail['InvoiceDate']
        rfm_p = retail.groupby('CustomerID')['Diff'].min().reset_index()
        rfm_p['Diff'] = rfm_p['Diff'].dt.days
        rfm_p.columns = ['CustomerID', 'Recency']
        rfm = pd.merge(rfm_m, rfm_f, on='CustomerID')
        rfm = pd.merge(rfm, rfm_p, on='CustomerID')
        rfm.columns = ['CustomerID', 'Amount', 'Frequency', 'Recency']
        Q1 = rfm.Amount.quantile(0.05)
        Q3 = rfm.Amount.quantile(0.95)
        IQR = Q3 - Q1
        rfm = rfm[(rfm.Amount >= Q1 - 1.5 * IQR) & (rfm.Amount <= Q3 + 1.5 * IQR)]
        Q1 = rfm.Recency.quantile(0.05)
        Q3 = rfm.Recency.quantile(0.95)
        IQR = Q3 - Q1
        rfm = rfm[(rfm        .Recency >= Q1 - 1.5 * IQR) & (rfm.Recency <= Q3 + 1.5 * IQR)]
        Q1 = rfm.Frequency.quantile(0.05)
        Q3 = rfm.Frequency.quantile(0.95)
        IQR = Q3 - Q1
        rfm = rfm[(rfm.Frequency >= Q1 - 1.5 * IQR) & (rfm.Frequency <= Q3 + 1.5 * IQR)]
        return rfm
    except Exception as e:
        st.error(f"Error loading and cleaning data: {e}")
        st.stop()

def preprocess_data(file_path):
    try:
        rfm = load_and_clean_data(file_path)
        rfm['CustomerID'] = rfm['CustomerID'].astype(int)
        rfm_df = rfm[['Amount', 'Frequency', 'Recency']]
        scaler = StandardScaler()
        rfm_df_scaled = scaler.fit_transform(rfm_df)
        rfm_df_scaled = pd.DataFrame(rfm_df_scaled)
        rfm_df_scaled.columns = ['Amount', 'Frequency', 'Recency']
        return rfm, rfm_df_scaled
    except Exception as e:
        st.error(f"Error preprocessing data: {e}")
        st.stop()

st.title('Customer Segmentation Using K-Means Clustering')

uploaded_file = st.file_uploader("Choose a CSV file", type="csv")

if uploaded_file is not None:
    try:
        file_path = os.path.join("/tmp", uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        rfm, df = preprocess_data(file_path)
        result_df = model.predict(df)
        rfm['Cluster_Id'] = result_df
        excel_file_path = os.path.join("/tmp", 'rfm_clusters.xlsx')
        rfm[['CustomerID', 'Cluster_Id']].to_excel(excel_file_path, index=False)

        st.write('Clustered Data')
        st.write(rfm[['CustomerID', 'Cluster_Id']])

        sns.stripplot(x='Cluster_Id', y='Amount', data=rfm, hue='Cluster_Id')
        st.pyplot()
        plt.clf()

        sns.stripplot(x='Cluster_Id', y='Frequency', data=rfm, hue='Cluster_Id')
        st.pyplot()
        plt.clf()

        sns.stripplot(x='Cluster_Id', y='Recency', data=rfm, hue='Cluster_Id')
        st.pyplot()
        plt.clf()

        with open(excel_file_path, "rb") as f:
            st.download_button(
                label="Download Excel file",
                data=f,
                file_name=excel_file_path,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
    except Exception as e:
        st.error(f"Error processing file: {e}")

def delete_old_files():
    while True:
        current_time = datetime.now()
        folder = "/tmp"  # This will cover all files in the temporary directory

        for filename in os.listdir(folder):
            file_path = os.path.join(folder, filename)

            # Check if the file is a relevant temporary file
            if (os.path.isfile(file_path) and
                (filename.endswith('.png') or filename.endswith('.xlsx') or filename.endswith('.csv'))):

                file_time = datetime.fromtimestamp(os.path.getmtime(file_path))

                if current_time - file_time > timedelta(minutes=3):
                    os.remove(file_path)

        time.sleep(60)  # Sleep for 60 seconds before checking again

# Start the file cleanup thread
Thread(target=delete_old_files, daemon=True).start()